{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carson-edmonds/AAI-511_Final_Project/blob/main/MSAAI_511_TEAM_3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSAAI 511 Neural Networks and Deep Learning\n",
        "# University of San Diego\n",
        "# Summer 2023 Section 02 Final Project Report\n",
        "# Professor: Dr. Mirsardar Esnaeilli\n",
        "## Final Project Team 3\n",
        "## Topic: Music Genre and Composer Classification Using Deep Learning\n",
        "\n",
        "## Project Team GitHub: https://github.com/carson-edmonds/AAI-511_Final_Project\n",
        "## Auguest 14, 2023"
      ],
      "metadata": {
        "id": "Ob5V2BPQkohD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Pre-processing:"
      ],
      "metadata": {
        "id": "9ldVHYqanUnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8h0z4BcVFzUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVg1AU8LF0dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Converting .midi files:\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "np.random.seed(42)  # makes the randomness deterministic\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15, 5)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "!pip install mido --quiet\n",
        "import mido\n",
        "import string\n",
        "\n",
        "def msg2dict(msg):\n",
        "    result = dict()\n",
        "    if 'note_on' in msg:\n",
        "        on_ = True\n",
        "    elif 'note_off' in msg:\n",
        "        on_ = False\n",
        "    else:\n",
        "        on_ = None\n",
        "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
        "        str.maketrans({a: None for a in string.punctuation})))\n",
        "\n",
        "    if on_ is not None:\n",
        "        for k in ['note', 'velocity']:\n",
        "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
        "                str.maketrans({a: None for a in string.punctuation})))\n",
        "    return [result, on_]\n",
        "\n",
        "def switch_note(last_state, note, velocity, on_=True):\n",
        "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
        "    result = [0] * 88 if last_state is None else last_state.copy()\n",
        "    if 21 <= note <= 108:\n",
        "        result[note-21] = velocity if on_ else 0\n",
        "    return result\n",
        "\n",
        "def get_new_state(new_msg, last_state):\n",
        "    new_msg, on_ = msg2dict(str(new_msg))\n",
        "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_) if on_ is not None else last_state\n",
        "    return [new_state, new_msg['time']]\n",
        "\n",
        "def track2seq(track):\n",
        "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of the id range will be ignored\n",
        "    result = []\n",
        "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
        "    for i in range(1, len(track)):\n",
        "        new_state, new_time = get_new_state(track[i], last_state)\n",
        "        if new_time > 0:\n",
        "            result += [last_state]*new_time\n",
        "        last_state, last_time = new_state, new_time\n",
        "    return result\n",
        "\n",
        "def mid2arry(mid, min_msg_pct=0.1):\n",
        "    tracks_len = [len(tr) for tr in mid.tracks]\n",
        "    min_n_msg = max(tracks_len) * min_msg_pct\n",
        "    # convert each track to nested list\n",
        "    all_arys = []\n",
        "    for i in range(len(mid.tracks)):\n",
        "        if len(mid.tracks[i]) > min_n_msg:\n",
        "            ary_i = track2seq(mid.tracks[i])\n",
        "            all_arys.append(ary_i)\n",
        "    # make all nested list the same length\n",
        "    max_len = max([len(ary) for ary in all_arys])\n",
        "    for i in range(len(all_arys)):\n",
        "        if len(all_arys[i]) < max_len:\n",
        "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i]))\n",
        "    all_arys = np.array(all_arys)\n",
        "    all_arys = all_arys.max(axis=0)\n",
        "    # trim: remove consecutive 0s in the beginning and at the end\n",
        "    sums = all_arys.sum(axis=1)\n",
        "    ends = np.where(sums > 0)[0]\n",
        "    return all_arys[min(ends): max(ends)]\n",
        "\n",
        "\n",
        "# The wrapper function to load a MIDI file and extract features\n",
        "def extract_features(file_path):\n",
        "    mid = mido.MidiFile(file_path, clip=True)\n",
        "\n",
        "    return mid2arry(mid)"
      ],
      "metadata": {
        "id": "q9jpS-TKFN2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify your directory containing MIDI folders\n",
        "main_dir = '/content/drive/MyDrive/Colab Notebooks/Data/Composer_Dataset/NN_midi_files_extended/train/'\n",
        "\n",
        "# Prepare lists to store filenames and lengths\n",
        "filenamelist = []\n",
        "lengths = []\n",
        "\n",
        "# Walk through all subdirectories\n",
        "for dirpath, dirnames, filenames in os.walk(main_dir):\n",
        "    for filename in tqdm(filenames):\n",
        "      if filename.endswith('.mid'):\n",
        "            full_file_path = os.path.join(dirpath, filename)  # get full file path\n",
        "            mid = mido.MidiFile(full_file_path, clip=True)\n",
        "            # Compute the total number of messages in all tracks\n",
        "            total_msgs = sum(len(track) for track in mid.tracks)\n",
        "            filenamelist.append(filename)\n",
        "            lengths.append(total_msgs)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'filename': filenamelist,\n",
        "    'length': lengths\n",
        "})"
      ],
      "metadata": {
        "id": "72MQKPedK_ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slVYkSD0kaI_"
      },
      "outputs": [],
      "source": [
        "#@title Processing Feature Extracted Data:\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#read csv files\n",
        "def csv_read(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_shape = df.shape\n",
        "  print(\"Shape of the dataframe (row, col):\", df_shape,\"\\r\\n\")\n",
        "  return df\n",
        "\n",
        "#encode object datatype data with labelencoder\n",
        "def df_encode(df):\n",
        "  obj_cols = list(df.select_dtypes(include='object'))\n",
        "  le = LabelEncoder()\n",
        "  df[obj_cols] = df[obj_cols].apply(LabelEncoder().fit_transform)\n",
        "  return df\n",
        "\n",
        "#reading all data subsets and encoding for model use\n",
        "def csv_to_df(train_path, val_path, test_path):\n",
        "  print(\"Train set:\")\n",
        "  train_df = csv_read(train_path)\n",
        "  print(\"Val set:\")\n",
        "  val_df = csv_read(val_path)\n",
        "  print(\"Test set:\")\n",
        "  test_df = csv_read(test_path)\n",
        "\n",
        "  train_df = df_encode(train_df)\n",
        "  val_df = df_encode(val_df)\n",
        "  test_df = df_encode(test_df)\n",
        "  return train_df, val_df, test_df\n",
        "\n",
        "#Using feature extracted datasets.\n",
        "#edit path to match correct file path:\n",
        "train_path = '/content/drive/MyDrive/AAI 511/Final Project/midi_features_traindataset_raw.csv'\n",
        "val_path = '/content/drive/MyDrive/AAI 511/Final Project/midi_features_valdataset_raw.csv'\n",
        "test_path = '/content/drive/MyDrive/AAI 511/Final Project/midi_features_testdataset_raw.csv'\n",
        "\n",
        "train_df, val_df, test_df = csv_to_df(train_path, val_path, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exploratory Data Analysis with prettymidi library:\n",
        "#prettymidi can be used for music data analysis\n",
        "!pip install pretty_midi --quiet\n",
        "import pretty_midi\n",
        "import librosa.display\n",
        "\n",
        "pm = pretty_midi.PrettyMIDI('/content/drive/MyDrive/AAI 511/Final Project/Composer_Dataset/NN_midi_files_extended/train/bartok/bartok396.mid')\n",
        "\n",
        "#plot Piano roll of song\n",
        "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plot_piano_roll(pm, 24, 84)\n",
        "\n",
        "# Plot a pitch class distribution\n",
        "plt.bar(np.arange(12), pm.get_pitch_class_histogram());\n",
        "plt.xticks(np.arange(12), ['C', '', 'D', '', 'E', 'F', '', 'G', '', 'A', '', 'B'])\n",
        "plt.xlabel('Note')\n",
        "plt.ylabel('Proportion')"
      ],
      "metadata": {
        "id": "a6IDAid6RM-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exploratory Data Analysis with music21 library:\n",
        "#Music21 can be used for music data analysis\n",
        "import music21\n",
        "from music21 import *\n",
        "\n",
        "#open/read a midi file\n",
        "def open_midi(midi_path, remove_drums):\n",
        "    mf = midi.MidiFile()\n",
        "    mf.open(midi_path)\n",
        "    mf.read()\n",
        "    mf.close()\n",
        "    if (remove_drums):\n",
        "        for i in range(len(mf.tracks)):\n",
        "            mf.tracks[i].events = [ev for ev in mf.tracks[i].events if ev.channel != 10]\n",
        "\n",
        "    return midi.translate.midiFileToStream(mf)\n",
        "\n",
        "base_midi = open_midi('/content/drive/MyDrive/AAI 511/Final Project/Composer_Dataset/NN_midi_files_extended/train/bartok/bartok396.mid', True)\n",
        "base_midi\n",
        "\n",
        "#list instruments within song\n",
        "def list_instruments(midi):\n",
        "    partStream = midi.parts.stream()\n",
        "    print(\"List of instruments found on MIDI file:\")\n",
        "    for p in partStream:\n",
        "        aux = p\n",
        "        print (p.partName)\n",
        "\n",
        "list_instruments(base_midi)\n",
        "\n",
        "\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "#extract notes from song\n",
        "def extract_notes(midi_part):\n",
        "    parent_element = []\n",
        "    ret = []\n",
        "    for nt in midi_part.flat.notes:\n",
        "        if isinstance(nt, note.Note):\n",
        "            ret.append(max(0.0, nt.pitch.ps))\n",
        "            parent_element.append(nt)\n",
        "        elif isinstance(nt, chord.Chord):\n",
        "            for pitch in nt.pitches:\n",
        "                ret.append(max(0.0, pitch.ps))\n",
        "                parent_element.append(nt)\n",
        "\n",
        "    return ret, parent_element\n",
        "\n",
        "def print_parts_countour(midi):\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    minPitch = pitch.Pitch('C10').ps\n",
        "    maxPitch = 0\n",
        "    xMax = 0\n",
        "\n",
        "# Plotting notes.\n",
        "    for i in range(len(midi.parts)):\n",
        "        top = midi.parts[i].flat.notes\n",
        "        y, parent_element = extract_notes(top)\n",
        "        if (len(y) < 1): continue\n",
        "\n",
        "        x = [n.offset for n in parent_element]\n",
        "        ax.scatter(x, y, alpha=0.6, s=7)\n",
        "\n",
        "        aux = min(y)\n",
        "        if (aux < minPitch): minPitch = aux\n",
        "\n",
        "        aux = max(y)\n",
        "        if (aux > maxPitch): maxPitch = aux\n",
        "\n",
        "        aux = max(x)\n",
        "        if (aux > xMax): xMax = aux\n",
        "\n",
        "    for i in range(1, 10):\n",
        "        linePitch = pitch.Pitch('C{0}'.format(i)).ps\n",
        "        if (linePitch > minPitch and linePitch < maxPitch):\n",
        "            ax.add_line(mlines.Line2D([0, xMax], [linePitch, linePitch], color='red', alpha=0.1))\n",
        "\n",
        "    plt.ylabel(\"Note index (each octave has 12 notes)\")\n",
        "    plt.xlabel(\"Number of quarter notes (beats)\")\n",
        "    plt.title('Voices motion approximation, each color is a different instrument, red lines show each octave')\n",
        "    plt.show()\n",
        "\n",
        "# Focusing only on 6 first measures.\n",
        "print_parts_countour(base_midi.measures(0, 6))\n",
        "\n",
        "#Plot pitch class\n",
        "base_midi.plot('histogram', 'pitchClass', 'count')\n",
        "\n",
        "#getting chords from song\n",
        "temp_midi_chords = base_midi.chordify()\n",
        "temp_midi = stream.Score()\n",
        "temp_midi.insert(0, temp_midi_chords)\n",
        "\n",
        "# Printing merged tracks.\n",
        "print_parts_countour(temp_midi)\n",
        "\n",
        "# Dumping first measure notes\n",
        "temp_midi_chords.measures(0, 1).show(\"text\")"
      ],
      "metadata": {
        "id": "SRlZvKmTLUTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exploratory Data Analysis with dataprep.eda:\n",
        "!pip install dataprep --quiet\n",
        "from dataprep.eda import create_report\n",
        "create_report(train_df)"
      ],
      "metadata": {
        "id": "cjyeJuydRTVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Extraction:"
      ],
      "metadata": {
        "id": "jLh9OiOhndfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Building:"
      ],
      "metadata": {
        "id": "8vT8G97TnhUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Training:"
      ],
      "metadata": {
        "id": "qSjBlWiLnmBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Evaluation:"
      ],
      "metadata": {
        "id": "T9T63CYcnqIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Optimization:"
      ],
      "metadata": {
        "id": "AaUYVuJqnwO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Conclusion and Discussion"
      ],
      "metadata": {
        "id": "EjJCEN8_n1Ld"
      }
    }
  ]
}